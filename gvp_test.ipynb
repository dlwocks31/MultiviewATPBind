{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17:58:50   ATPBind3D: loaded 20 proteins and 20 targets\n",
      "17:58:53   ATPBind3D: loaded 20 gvp graphs. length of data: 20\n",
      "Initialize Undersampling: all ones\n",
      "Initialize Weighting: all ones\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<torch.utils.data.dataset.Subset at 0x7f0061e47910>,\n",
       " <torch.utils.data.dataset.Subset at 0x7f0061e47790>,\n",
       " <torch.utils.data.dataset.Subset at 0x7f0061e47700>]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.datasets import ATPBind3D\n",
    "dataset = ATPBind3D(limit=10).initialize_mask_and_weights()\n",
    "\n",
    "dataset.split(valid_fold_num=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'graph': Protein(num_atom=2264, num_bond=4524, num_residue=566),\n",
       " 'gvp_data': Data(x=[566, 3], edge_index=[2, 16980], seq=[566], name='', node_s=[566, 6], node_v=[566, 3, 3], edge_s=[16980, 32], edge_v=[16980, 1, 3], mask=[566])}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.get_item(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graph: <class 'torchdrug.data.protein.PackedProtein'>\n",
      "gvp_data: <class 'torch_geometric.data.batch.DataBatch'>\n",
      "\n",
      "Accessing specific elements:\n",
      "Graph: PackedProtein(batch_size=1, num_atoms=[2264], num_bonds=[4524], num_residues=[566], device='cuda:0')\n",
      "GVP data: DataBatch(x=[566, 3], edge_index=[2, 16980], seq=[566], name=[1], node_s=[566, 6], node_v=[566, 3, 3], edge_s=[16980, 32], edge_v=[16980, 1, 3], mask=[566], batch=[566], ptr=[2])\n"
     ]
    }
   ],
   "source": [
    "from torchdrug import data, utils\n",
    "from lib.pipeline import graph_collate_with_gvp\n",
    "import torch\n",
    "\n",
    "# Create a DataLoader for the dataset\n",
    "batch_size = 1  # You can adjust this as needed\n",
    "dataloader = data.DataLoader(dataset, batch_size=batch_size, shuffle=False, collate_fn=graph_collate_with_gvp)\n",
    "\n",
    "# Retrieve a single batch\n",
    "batch = next(iter(dataloader))\n",
    "batch = utils.cuda(batch)\n",
    "for key, value in batch.items():\n",
    "    if isinstance(value, torch.Tensor):\n",
    "        print(f\"{key}: {value.shape}\")\n",
    "    else:\n",
    "        print(f\"{key}: {type(value)}\")\n",
    "\n",
    "# Access specific elements of the batch\n",
    "print(\"\\nAccessing specific elements:\")\n",
    "print(\"Graph:\", batch['graph'])\n",
    "print(\"GVP data:\", batch['gvp_data'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([17,  1,  0, 26, 17,  1,  0, 26], device='cuda:0')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['graph'].atom_name[:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.Protein.atom_name2id[\"CA\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 0, 1, 1, 1, 1, 2, 2, 2, 2], device='cuda:0')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['graph'].atom2residue[0:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['atom',\n",
       " 'atom2graph',\n",
       " 'atom2residue',\n",
       " 'atom2valence',\n",
       " 'atom_map',\n",
       " 'atom_name',\n",
       " 'atom_name2id',\n",
       " 'atom_reference',\n",
       " 'atom_type']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in dir(batch['graph']) if i.startswith('atom')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2264, 3])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['graph'].node_position.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([566, 21])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['graph'].residue_feature.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node positions shape: torch.Size([2264, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'PRGLELLIAQTILQGFDAQYGRFLEVTSGAQQRFEQADWHAVQQAMKNRIHLYDHHVGLVVEQLRCITNGQSTDAEFLLRVKEHYTRLLPDYPRFEIAESFFNSVYCRLFDHRSLTPERLFIFSSQPERRFRTIPRPLAKDFHPDHGWESLLMRVISDLPLRLHWQNKSRDIHYIIRHLTETLGPENLSKSHLQVANELFYRNKAAWLVGKLITPSGTLPFLLPIHQTDDGELFIDTCLTTTAEASIVFGFARSYFMVYAPLPAALVEWLREILPGKTTAELYMAIGCQKHAKTESYREYLVYLQGCNEQFIEAPGIRGMVMLVFTLPGFDRVFKVIKDKFAPQKEMSAAHVRACYQLVKEHDRVGRMADTQEFENFVLEKRHISPALMELLLQEAAEKITDLGEQIVIRHLYIERRMVPLNIWLEQVEGQQLRDAIEEYGNAIRQLAAANIFPGDMLFKNFGVTRHGRVVFYDYDEICYMTEVNFRDIPPPRYP.PWYSVSPGDVFPEEFRHWLCADPRIGPLFEEMHADLFRADYWRALQNRIREGHVEDVYAYRRRQRFSVRYG'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the first protein from batch['graph']\n",
    "import numpy as np\n",
    "from Bio.PDB.Polypeptide import protein_letters_3to1\n",
    "first_protein = batch['graph'][0]\n",
    "\n",
    "print(f\"Node positions shape: {first_protein.node_position.shape}\")\n",
    "protein = first_protein\n",
    "\n",
    "protein.to_sequence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_residue_atom_data(protein, residue_index):\n",
    "    \"\"\"Extract atom data for a single residue.\"\"\"\n",
    "    atom_indices = protein.residue2atom(residue_index)\n",
    "    atom_positions = protein.node_position[atom_indices]\n",
    "    atom_name_ids = protein.atom_name[atom_indices]\n",
    "    return atom_positions, atom_name_ids\n",
    "\n",
    "def get_residue_coords(protein, residue_index, target_atoms):\n",
    "    \"\"\"Extract coordinates for target atoms of a single residue.\"\"\"\n",
    "    atom_positions, atom_name_ids = get_residue_atom_data(protein, residue_index)\n",
    "    \n",
    "    residue_coords = []\n",
    "    existing_coords = []\n",
    "    for target_atom in target_atoms:\n",
    "        target_atom_id = data.Protein.atom_name2id[target_atom]\n",
    "        matching_positions = atom_positions[(atom_name_ids == target_atom_id).nonzero()[0][0]]\n",
    "        \n",
    "        if matching_positions.numel() > 0:\n",
    "            coord = matching_positions.cpu().numpy()\n",
    "            residue_coords.append(coord)\n",
    "            existing_coords.append(coord)\n",
    "        else:\n",
    "            residue_coords.append(None)\n",
    "    \n",
    "    # Fallback to mean coordinate if some atoms are missing\n",
    "    if len(existing_coords) > 0:\n",
    "        mean_coord = np.mean(existing_coords, axis=0)\n",
    "        residue_coords = [coord if coord is not None else mean_coord for coord in residue_coords]\n",
    "    else:\n",
    "        raise ValueError(f\"No coordinates found for residue {residue_index+1}\")\n",
    "    \n",
    "    return residue_coords\n",
    "\n",
    "def parse_protein_to_json_record(protein, name):\n",
    "    \"\"\"Convert a torchprotein Protein structure to coordinates of target atoms from all AAs\n",
    "\n",
    "    Args:\n",
    "        protein: a torchprotein.Protein object representing the protein structure\n",
    "        name: String. Name of the protein\n",
    "\n",
    "    Return:\n",
    "        Dictionary with the protein sequence, atom 3D coordinates and name.\n",
    "    \"\"\"\n",
    "    output = {}\n",
    "    \n",
    "    # Get AA sequence\n",
    "    output[\"seq\"] = protein.to_sequence()\n",
    "    \n",
    "    # Get atom coordinates\n",
    "    coords = []\n",
    "    target_atoms = [\"N\", \"CA\", \"C\", \"O\"]\n",
    "    for residue_index in range(protein.num_residue):\n",
    "        residue_coords = get_residue_coords(protein, residue_index, target_atoms)\n",
    "        coords.append(residue_coords)\n",
    "    \n",
    "    coords = np.asarray(coords)\n",
    "    \n",
    "    output[\"coords\"] = coords\n",
    "    output[\"name\"] = name\n",
    "    \n",
    "    return output\n",
    "\n",
    "record = parse_protein_to_json_record(protein, name='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.001078 seconds\n",
      "Result: tensor([17,  1,  0, 26], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "start_time = time.time()\n",
    "result = protein.atom_name[protein.atom2residue == 1]\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time:.6f} seconds\")\n",
    "print(\"Result:\", result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 33.5700,  19.2910, -10.2110],\n",
       "        [ 34.5020,  18.2620, -10.6680],\n",
       "        [ 35.8620,  18.3450,  -9.9930],\n",
       "        [ 36.3470,  17.3590,  -9.4360]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "protein.node_position[protein.atom2residue == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GVPWrapModel(\n",
       "  (W_v): Sequential(\n",
       "    (0): GVP(\n",
       "      (wh): Linear(in_features=3, out_features=128, bias=False)\n",
       "      (ws): Linear(in_features=134, out_features=128, bias=True)\n",
       "      (wv): Linear(in_features=128, out_features=128, bias=False)\n",
       "    )\n",
       "    (1): LayerNorm(\n",
       "      (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (W_e): Sequential(\n",
       "    (0): GVP(\n",
       "      (wh): Linear(in_features=1, out_features=128, bias=False)\n",
       "      (ws): Linear(in_features=160, out_features=128, bias=True)\n",
       "      (wv): Linear(in_features=128, out_features=128, bias=False)\n",
       "    )\n",
       "    (1): LayerNorm(\n",
       "      (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (encoder_layers): ModuleList(\n",
       "    (0): GVPConvLayer(\n",
       "      (conv): GVPConv()\n",
       "      (norm): ModuleList(\n",
       "        (0): LayerNorm(\n",
       "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): LayerNorm(\n",
       "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): ModuleList(\n",
       "        (0): Dropout(\n",
       "          (sdropout): Dropout(p=0.1, inplace=False)\n",
       "          (vdropout): _VDropout()\n",
       "        )\n",
       "        (1): Dropout(\n",
       "          (sdropout): Dropout(p=0.1, inplace=False)\n",
       "          (vdropout): _VDropout()\n",
       "        )\n",
       "      )\n",
       "      (ff_func): Sequential(\n",
       "        (0): GVP(\n",
       "          (wh): Linear(in_features=128, out_features=256, bias=False)\n",
       "          (ws): Linear(in_features=384, out_features=512, bias=True)\n",
       "          (wv): Linear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "        (1): GVP(\n",
       "          (wh): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (ws): Linear(in_features=768, out_features=128, bias=True)\n",
       "          (wv): Linear(in_features=256, out_features=128, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): GVPConvLayer(\n",
       "      (conv): GVPConv()\n",
       "      (norm): ModuleList(\n",
       "        (0): LayerNorm(\n",
       "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): LayerNorm(\n",
       "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): ModuleList(\n",
       "        (0): Dropout(\n",
       "          (sdropout): Dropout(p=0.1, inplace=False)\n",
       "          (vdropout): _VDropout()\n",
       "        )\n",
       "        (1): Dropout(\n",
       "          (sdropout): Dropout(p=0.1, inplace=False)\n",
       "          (vdropout): _VDropout()\n",
       "        )\n",
       "      )\n",
       "      (ff_func): Sequential(\n",
       "        (0): GVP(\n",
       "          (wh): Linear(in_features=128, out_features=256, bias=False)\n",
       "          (ws): Linear(in_features=384, out_features=512, bias=True)\n",
       "          (wv): Linear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "        (1): GVP(\n",
       "          (wh): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (ws): Linear(in_features=768, out_features=128, bias=True)\n",
       "          (wv): Linear(in_features=256, out_features=128, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): GVPConvLayer(\n",
       "      (conv): GVPConv()\n",
       "      (norm): ModuleList(\n",
       "        (0): LayerNorm(\n",
       "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): LayerNorm(\n",
       "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): ModuleList(\n",
       "        (0): Dropout(\n",
       "          (sdropout): Dropout(p=0.1, inplace=False)\n",
       "          (vdropout): _VDropout()\n",
       "        )\n",
       "        (1): Dropout(\n",
       "          (sdropout): Dropout(p=0.1, inplace=False)\n",
       "          (vdropout): _VDropout()\n",
       "        )\n",
       "      )\n",
       "      (ff_func): Sequential(\n",
       "        (0): GVP(\n",
       "          (wh): Linear(in_features=128, out_features=256, bias=False)\n",
       "          (ws): Linear(in_features=384, out_features=512, bias=True)\n",
       "          (wv): Linear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "        (1): GVP(\n",
       "          (wh): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (ws): Linear(in_features=768, out_features=128, bias=True)\n",
       "          (wv): Linear(in_features=256, out_features=128, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (W_s): Embedding(20, 20)\n",
       "  (decoder_layers): ModuleList(\n",
       "    (0): GVPConvLayer(\n",
       "      (conv): GVPConv()\n",
       "      (norm): ModuleList(\n",
       "        (0): LayerNorm(\n",
       "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): LayerNorm(\n",
       "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): ModuleList(\n",
       "        (0): Dropout(\n",
       "          (sdropout): Dropout(p=0.1, inplace=False)\n",
       "          (vdropout): _VDropout()\n",
       "        )\n",
       "        (1): Dropout(\n",
       "          (sdropout): Dropout(p=0.1, inplace=False)\n",
       "          (vdropout): _VDropout()\n",
       "        )\n",
       "      )\n",
       "      (ff_func): Sequential(\n",
       "        (0): GVP(\n",
       "          (wh): Linear(in_features=128, out_features=256, bias=False)\n",
       "          (ws): Linear(in_features=384, out_features=512, bias=True)\n",
       "          (wv): Linear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "        (1): GVP(\n",
       "          (wh): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (ws): Linear(in_features=768, out_features=128, bias=True)\n",
       "          (wv): Linear(in_features=256, out_features=128, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (1): GVPConvLayer(\n",
       "      (conv): GVPConv()\n",
       "      (norm): ModuleList(\n",
       "        (0): LayerNorm(\n",
       "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): LayerNorm(\n",
       "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): ModuleList(\n",
       "        (0): Dropout(\n",
       "          (sdropout): Dropout(p=0.1, inplace=False)\n",
       "          (vdropout): _VDropout()\n",
       "        )\n",
       "        (1): Dropout(\n",
       "          (sdropout): Dropout(p=0.1, inplace=False)\n",
       "          (vdropout): _VDropout()\n",
       "        )\n",
       "      )\n",
       "      (ff_func): Sequential(\n",
       "        (0): GVP(\n",
       "          (wh): Linear(in_features=128, out_features=256, bias=False)\n",
       "          (ws): Linear(in_features=384, out_features=512, bias=True)\n",
       "          (wv): Linear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "        (1): GVP(\n",
       "          (wh): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (ws): Linear(in_features=768, out_features=128, bias=True)\n",
       "          (wv): Linear(in_features=256, out_features=128, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (2): GVPConvLayer(\n",
       "      (conv): GVPConv()\n",
       "      (norm): ModuleList(\n",
       "        (0): LayerNorm(\n",
       "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (1): LayerNorm(\n",
       "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (dropout): ModuleList(\n",
       "        (0): Dropout(\n",
       "          (sdropout): Dropout(p=0.1, inplace=False)\n",
       "          (vdropout): _VDropout()\n",
       "        )\n",
       "        (1): Dropout(\n",
       "          (sdropout): Dropout(p=0.1, inplace=False)\n",
       "          (vdropout): _VDropout()\n",
       "        )\n",
       "      )\n",
       "      (ff_func): Sequential(\n",
       "        (0): GVP(\n",
       "          (wh): Linear(in_features=128, out_features=256, bias=False)\n",
       "          (ws): Linear(in_features=384, out_features=512, bias=True)\n",
       "          (wv): Linear(in_features=256, out_features=256, bias=False)\n",
       "        )\n",
       "        (1): GVP(\n",
       "          (wh): Linear(in_features=256, out_features=256, bias=False)\n",
       "          (ws): Linear(in_features=768, out_features=128, bias=True)\n",
       "          (wv): Linear(in_features=256, out_features=128, bias=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (W_out): GVP(\n",
       "    (wh): Linear(in_features=128, out_features=128, bias=False)\n",
       "    (ws): Linear(in_features=256, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.custom_models import GVPWrapModel\n",
    "\n",
    "model = GVPWrapModel(node_in_dim=(6, 3), node_h_dim=(128, 128), edge_in_dim=(\n",
    "    32, 1), edge_h_dim=(128, 128), num_layers=3, drop_rate=0.1, gpu=0)\n",
    "model.to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init pipeline, model: GVPWrapModel(\n",
      "  (W_v): Sequential(\n",
      "    (0): GVP(\n",
      "      (wh): Linear(in_features=3, out_features=128, bias=False)\n",
      "      (ws): Linear(in_features=134, out_features=128, bias=True)\n",
      "      (wv): Linear(in_features=128, out_features=128, bias=False)\n",
      "    )\n",
      "    (1): LayerNorm(\n",
      "      (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (W_e): Sequential(\n",
      "    (0): GVP(\n",
      "      (wh): Linear(in_features=1, out_features=128, bias=False)\n",
      "      (ws): Linear(in_features=160, out_features=128, bias=True)\n",
      "      (wv): Linear(in_features=128, out_features=128, bias=False)\n",
      "    )\n",
      "    (1): LayerNorm(\n",
      "      (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layers): ModuleList(\n",
      "    (0): GVPConvLayer(\n",
      "      (conv): GVPConv()\n",
      "      (norm): ModuleList(\n",
      "        (0): LayerNorm(\n",
      "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): LayerNorm(\n",
      "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): ModuleList(\n",
      "        (0): Dropout(\n",
      "          (sdropout): Dropout(p=0.1, inplace=False)\n",
      "          (vdropout): _VDropout()\n",
      "        )\n",
      "        (1): Dropout(\n",
      "          (sdropout): Dropout(p=0.1, inplace=False)\n",
      "          (vdropout): _VDropout()\n",
      "        )\n",
      "      )\n",
      "      (ff_func): Sequential(\n",
      "        (0): GVP(\n",
      "          (wh): Linear(in_features=128, out_features=256, bias=False)\n",
      "          (ws): Linear(in_features=384, out_features=512, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (1): GVP(\n",
      "          (wh): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (ws): Linear(in_features=768, out_features=128, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): GVPConvLayer(\n",
      "      (conv): GVPConv()\n",
      "      (norm): ModuleList(\n",
      "        (0): LayerNorm(\n",
      "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): LayerNorm(\n",
      "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): ModuleList(\n",
      "        (0): Dropout(\n",
      "          (sdropout): Dropout(p=0.1, inplace=False)\n",
      "          (vdropout): _VDropout()\n",
      "        )\n",
      "        (1): Dropout(\n",
      "          (sdropout): Dropout(p=0.1, inplace=False)\n",
      "          (vdropout): _VDropout()\n",
      "        )\n",
      "      )\n",
      "      (ff_func): Sequential(\n",
      "        (0): GVP(\n",
      "          (wh): Linear(in_features=128, out_features=256, bias=False)\n",
      "          (ws): Linear(in_features=384, out_features=512, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (1): GVP(\n",
      "          (wh): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (ws): Linear(in_features=768, out_features=128, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): GVPConvLayer(\n",
      "      (conv): GVPConv()\n",
      "      (norm): ModuleList(\n",
      "        (0): LayerNorm(\n",
      "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): LayerNorm(\n",
      "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): ModuleList(\n",
      "        (0): Dropout(\n",
      "          (sdropout): Dropout(p=0.1, inplace=False)\n",
      "          (vdropout): _VDropout()\n",
      "        )\n",
      "        (1): Dropout(\n",
      "          (sdropout): Dropout(p=0.1, inplace=False)\n",
      "          (vdropout): _VDropout()\n",
      "        )\n",
      "      )\n",
      "      (ff_func): Sequential(\n",
      "        (0): GVP(\n",
      "          (wh): Linear(in_features=128, out_features=256, bias=False)\n",
      "          (ws): Linear(in_features=384, out_features=512, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (1): GVP(\n",
      "          (wh): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (ws): Linear(in_features=768, out_features=128, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (W_s): Embedding(20, 20)\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0): GVPConvLayer(\n",
      "      (conv): GVPConv()\n",
      "      (norm): ModuleList(\n",
      "        (0): LayerNorm(\n",
      "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): LayerNorm(\n",
      "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): ModuleList(\n",
      "        (0): Dropout(\n",
      "          (sdropout): Dropout(p=0.1, inplace=False)\n",
      "          (vdropout): _VDropout()\n",
      "        )\n",
      "        (1): Dropout(\n",
      "          (sdropout): Dropout(p=0.1, inplace=False)\n",
      "          (vdropout): _VDropout()\n",
      "        )\n",
      "      )\n",
      "      (ff_func): Sequential(\n",
      "        (0): GVP(\n",
      "          (wh): Linear(in_features=128, out_features=256, bias=False)\n",
      "          (ws): Linear(in_features=384, out_features=512, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (1): GVP(\n",
      "          (wh): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (ws): Linear(in_features=768, out_features=128, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): GVPConvLayer(\n",
      "      (conv): GVPConv()\n",
      "      (norm): ModuleList(\n",
      "        (0): LayerNorm(\n",
      "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): LayerNorm(\n",
      "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): ModuleList(\n",
      "        (0): Dropout(\n",
      "          (sdropout): Dropout(p=0.1, inplace=False)\n",
      "          (vdropout): _VDropout()\n",
      "        )\n",
      "        (1): Dropout(\n",
      "          (sdropout): Dropout(p=0.1, inplace=False)\n",
      "          (vdropout): _VDropout()\n",
      "        )\n",
      "      )\n",
      "      (ff_func): Sequential(\n",
      "        (0): GVP(\n",
      "          (wh): Linear(in_features=128, out_features=256, bias=False)\n",
      "          (ws): Linear(in_features=384, out_features=512, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (1): GVP(\n",
      "          (wh): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (ws): Linear(in_features=768, out_features=128, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): GVPConvLayer(\n",
      "      (conv): GVPConv()\n",
      "      (norm): ModuleList(\n",
      "        (0): LayerNorm(\n",
      "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): LayerNorm(\n",
      "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): ModuleList(\n",
      "        (0): Dropout(\n",
      "          (sdropout): Dropout(p=0.1, inplace=False)\n",
      "          (vdropout): _VDropout()\n",
      "        )\n",
      "        (1): Dropout(\n",
      "          (sdropout): Dropout(p=0.1, inplace=False)\n",
      "          (vdropout): _VDropout()\n",
      "        )\n",
      "      )\n",
      "      (ff_func): Sequential(\n",
      "        (0): GVP(\n",
      "          (wh): Linear(in_features=128, out_features=256, bias=False)\n",
      "          (ws): Linear(in_features=384, out_features=512, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (1): GVP(\n",
      "          (wh): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (ws): Linear(in_features=768, out_features=128, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (W_out): GVP(\n",
      "    (wh): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (ws): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "), dataset: atpbind3d-minimal, gpus: [0]\n",
      "load model GVPWrapModel(\n",
      "  (W_v): Sequential(\n",
      "    (0): GVP(\n",
      "      (wh): Linear(in_features=3, out_features=128, bias=False)\n",
      "      (ws): Linear(in_features=134, out_features=128, bias=True)\n",
      "      (wv): Linear(in_features=128, out_features=128, bias=False)\n",
      "    )\n",
      "    (1): LayerNorm(\n",
      "      (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (W_e): Sequential(\n",
      "    (0): GVP(\n",
      "      (wh): Linear(in_features=1, out_features=128, bias=False)\n",
      "      (ws): Linear(in_features=160, out_features=128, bias=True)\n",
      "      (wv): Linear(in_features=128, out_features=128, bias=False)\n",
      "    )\n",
      "    (1): LayerNorm(\n",
      "      (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "    )\n",
      "  )\n",
      "  (encoder_layers): ModuleList(\n",
      "    (0): GVPConvLayer(\n",
      "      (conv): GVPConv()\n",
      "      (norm): ModuleList(\n",
      "        (0): LayerNorm(\n",
      "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): LayerNorm(\n",
      "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): ModuleList(\n",
      "        (0): Dropout(\n",
      "          (sdropout): Dropout(p=0.1, inplace=False)\n",
      "          (vdropout): _VDropout()\n",
      "        )\n",
      "        (1): Dropout(\n",
      "          (sdropout): Dropout(p=0.1, inplace=False)\n",
      "          (vdropout): _VDropout()\n",
      "        )\n",
      "      )\n",
      "      (ff_func): Sequential(\n",
      "        (0): GVP(\n",
      "          (wh): Linear(in_features=128, out_features=256, bias=False)\n",
      "          (ws): Linear(in_features=384, out_features=512, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (1): GVP(\n",
      "          (wh): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (ws): Linear(in_features=768, out_features=128, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): GVPConvLayer(\n",
      "      (conv): GVPConv()\n",
      "      (norm): ModuleList(\n",
      "        (0): LayerNorm(\n",
      "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): LayerNorm(\n",
      "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): ModuleList(\n",
      "        (0): Dropout(\n",
      "          (sdropout): Dropout(p=0.1, inplace=False)\n",
      "          (vdropout): _VDropout()\n",
      "        )\n",
      "        (1): Dropout(\n",
      "          (sdropout): Dropout(p=0.1, inplace=False)\n",
      "          (vdropout): _VDropout()\n",
      "        )\n",
      "      )\n",
      "      (ff_func): Sequential(\n",
      "        (0): GVP(\n",
      "          (wh): Linear(in_features=128, out_features=256, bias=False)\n",
      "          (ws): Linear(in_features=384, out_features=512, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (1): GVP(\n",
      "          (wh): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (ws): Linear(in_features=768, out_features=128, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): GVPConvLayer(\n",
      "      (conv): GVPConv()\n",
      "      (norm): ModuleList(\n",
      "        (0): LayerNorm(\n",
      "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): LayerNorm(\n",
      "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): ModuleList(\n",
      "        (0): Dropout(\n",
      "          (sdropout): Dropout(p=0.1, inplace=False)\n",
      "          (vdropout): _VDropout()\n",
      "        )\n",
      "        (1): Dropout(\n",
      "          (sdropout): Dropout(p=0.1, inplace=False)\n",
      "          (vdropout): _VDropout()\n",
      "        )\n",
      "      )\n",
      "      (ff_func): Sequential(\n",
      "        (0): GVP(\n",
      "          (wh): Linear(in_features=128, out_features=256, bias=False)\n",
      "          (ws): Linear(in_features=384, out_features=512, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (1): GVP(\n",
      "          (wh): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (ws): Linear(in_features=768, out_features=128, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (W_s): Embedding(20, 20)\n",
      "  (decoder_layers): ModuleList(\n",
      "    (0): GVPConvLayer(\n",
      "      (conv): GVPConv()\n",
      "      (norm): ModuleList(\n",
      "        (0): LayerNorm(\n",
      "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): LayerNorm(\n",
      "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): ModuleList(\n",
      "        (0): Dropout(\n",
      "          (sdropout): Dropout(p=0.1, inplace=False)\n",
      "          (vdropout): _VDropout()\n",
      "        )\n",
      "        (1): Dropout(\n",
      "          (sdropout): Dropout(p=0.1, inplace=False)\n",
      "          (vdropout): _VDropout()\n",
      "        )\n",
      "      )\n",
      "      (ff_func): Sequential(\n",
      "        (0): GVP(\n",
      "          (wh): Linear(in_features=128, out_features=256, bias=False)\n",
      "          (ws): Linear(in_features=384, out_features=512, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (1): GVP(\n",
      "          (wh): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (ws): Linear(in_features=768, out_features=128, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): GVPConvLayer(\n",
      "      (conv): GVPConv()\n",
      "      (norm): ModuleList(\n",
      "        (0): LayerNorm(\n",
      "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): LayerNorm(\n",
      "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): ModuleList(\n",
      "        (0): Dropout(\n",
      "          (sdropout): Dropout(p=0.1, inplace=False)\n",
      "          (vdropout): _VDropout()\n",
      "        )\n",
      "        (1): Dropout(\n",
      "          (sdropout): Dropout(p=0.1, inplace=False)\n",
      "          (vdropout): _VDropout()\n",
      "        )\n",
      "      )\n",
      "      (ff_func): Sequential(\n",
      "        (0): GVP(\n",
      "          (wh): Linear(in_features=128, out_features=256, bias=False)\n",
      "          (ws): Linear(in_features=384, out_features=512, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (1): GVP(\n",
      "          (wh): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (ws): Linear(in_features=768, out_features=128, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (2): GVPConvLayer(\n",
      "      (conv): GVPConv()\n",
      "      (norm): ModuleList(\n",
      "        (0): LayerNorm(\n",
      "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): LayerNorm(\n",
      "          (scalar_norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "      (dropout): ModuleList(\n",
      "        (0): Dropout(\n",
      "          (sdropout): Dropout(p=0.1, inplace=False)\n",
      "          (vdropout): _VDropout()\n",
      "        )\n",
      "        (1): Dropout(\n",
      "          (sdropout): Dropout(p=0.1, inplace=False)\n",
      "          (vdropout): _VDropout()\n",
      "        )\n",
      "      )\n",
      "      (ff_func): Sequential(\n",
      "        (0): GVP(\n",
      "          (wh): Linear(in_features=128, out_features=256, bias=False)\n",
      "          (ws): Linear(in_features=384, out_features=512, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=256, bias=False)\n",
      "        )\n",
      "        (1): GVP(\n",
      "          (wh): Linear(in_features=256, out_features=256, bias=False)\n",
      "          (ws): Linear(in_features=768, out_features=128, bias=True)\n",
      "          (wv): Linear(in_features=256, out_features=128, bias=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (W_out): GVP(\n",
      "    (wh): Linear(in_features=128, out_features=128, bias=False)\n",
      "    (ws): Linear(in_features=256, out_features=1, bias=True)\n",
      "  )\n",
      "), kwargs: {}\n",
      "get dataset with kwargs: {'to_slice': True, 'max_slice_length': 550, 'padding': 100}\n",
      "get dataset atpbind3d-minimal\n",
      "17:58:57   ATPBind3D: loaded 10 proteins and 10 targets\n",
      "17:58:57   ATPBind3D: sliced original data into 6 proteins\n",
      "17:58:58   ATPBind3D: loaded 11 gvp graphs. length of data: 11\n",
      "Initialize Undersampling: all ones\n",
      "Initialize Weighting: all ones\n",
      "train samples: 4, valid samples: 2, test samples: 5\n",
      "no scheduler\n",
      "pipeline batch_size: 1, gradient_interval: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8254],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8339],\n",
       "        [0.8341],\n",
       "        [0.8330],\n",
       "        [0.8345],\n",
       "        [0.8339],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8299],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8315],\n",
       "        [0.8251],\n",
       "        [0.8345],\n",
       "        [0.8284],\n",
       "        [0.8345],\n",
       "        [0.8335],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8322],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8324],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8293],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8292],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8283],\n",
       "        [0.8329],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8260],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8315],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8323],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8341],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8330],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8325],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8286],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8341],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8336],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8322],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8301],\n",
       "        [0.8324],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8336],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8312],\n",
       "        [0.8345],\n",
       "        [0.8343],\n",
       "        [0.8330],\n",
       "        [0.8345],\n",
       "        [0.8340],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8318],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8299],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8275],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8337],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8342],\n",
       "        [0.8288],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8312],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8312],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8262],\n",
       "        [0.8345],\n",
       "        [0.8332],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8337],\n",
       "        [0.8345],\n",
       "        [0.8344],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8336],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8336],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8258],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8296],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8337],\n",
       "        [0.8345],\n",
       "        [0.8306],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8296],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8276],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8284],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8322],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8330],\n",
       "        [0.8345],\n",
       "        [0.8345],\n",
       "        [0.8203],\n",
       "        [0.8345],\n",
       "        [0.8345]], device='cuda:0', grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lib.pipeline import Pipeline\n",
    "\n",
    "pipeline_kwargs = {\n",
    "    'model': model,\n",
    "    'dataset': 'atpbind3d-minimal',\n",
    "    'gpus': [0],\n",
    "    'model_kwargs': {},\n",
    "    'optimizer_kwargs': {'lr': 1e-4},\n",
    "    'task_kwargs': {\n",
    "        'normalization': False,\n",
    "        'num_mlp_layer': 2,\n",
    "        'metric': ['mcc'],\n",
    "        'node_feature_type': 'gvp_data',\n",
    "    },\n",
    "    'batch_size': 1,\n",
    "    'gradient_interval': 1,\n",
    "    'verbose': False,\n",
    "    'valid_fold_num': 0,\n",
    "    'dataset_kwargs': {\n",
    "        'to_slice': True,\n",
    "        'max_slice_length': 550,\n",
    "        'padding': 100\n",
    "    },\n",
    "    'num_mlp_layer': 2,\n",
    "}\n",
    "\n",
    "pipeline = Pipeline(**pipeline_kwargs)\n",
    "task = pipeline.task\n",
    "\n",
    "# Test the task\n",
    "task.predict(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init pipeline, model: gearnet, dataset: atpbind3d-minimal, gpus: [0]\n",
      "load model gearnet, kwargs: {'input_dim': 21, 'hidden_dims': [512, 512, 512, 512], 'gpu': 0}\n",
      "get dataset with kwargs: {'to_slice': True, 'max_slice_length': 550, 'padding': 100}\n",
      "Initialize Undersampling: all ones\n",
      "Initialize Weighting: all ones\n",
      "train samples: 4, valid samples: 2, test samples: 5\n",
      "no scheduler\n",
      "pipeline batch_size: 1, gradient_interval: 1\n",
      "0m2s {'mcc': -0.0068, 'valid_mcc': 0.0, 'train_bce': 0.9336, 'valid_bce': 38.4902, 'best_threshold': -3.0}\n",
      "0m1s {'mcc': -0.0136, 'valid_mcc': 0.0, 'train_bce': 0.4173, 'valid_bce': 4.5143, 'best_threshold': -1.8}\n",
      "0m1s {'mcc': 0.0, 'valid_mcc': 0.0, 'train_bce': 0.1301, 'valid_bce': 0.5955, 'best_threshold': 0.3}\n",
      "0m1s {'mcc': 0.1262, 'valid_mcc': 0.1005, 'train_bce': 0.0937, 'valid_bce': 0.3288, 'best_threshold': -1.5}\n",
      "0m1s {'mcc': 0.1081, 'valid_mcc': 0.0767, 'train_bce': 0.022, 'valid_bce': 0.3913, 'best_threshold': -0.8}\n",
      "0m1s {'mcc': 0.1094, 'valid_mcc': 0.0641, 'train_bce': 0.0159, 'valid_bce': 0.3843, 'best_threshold': -1.4}\n",
      "0m1s {'mcc': 0.1151, 'valid_mcc': 0.0978, 'train_bce': 0.0063, 'valid_bce': 0.343, 'best_threshold': -2.1}\n",
      "0m1s {'mcc': 0.1422, 'valid_mcc': 0.15, 'train_bce': 0.0063, 'valid_bce': 0.3335, 'best_threshold': -2.6}\n",
      "0m1s {'mcc': 0.1537, 'valid_mcc': 0.1973, 'train_bce': 0.0024, 'valid_bce': 0.3702, 'best_threshold': -1.2}\n",
      "0m1s {'mcc': 0.1302, 'valid_mcc': 0.1514, 'train_bce': 0.0011, 'valid_bce': 0.4062, 'best_threshold': -1.3}\n"
     ]
    }
   ],
   "source": [
    "pipeline_kwargs = {\n",
    "    'model': 'gearnet',\n",
    "    'dataset': 'atpbind3d-minimal',\n",
    "    'gpus': [0],\n",
    "    'model_kwargs': {\n",
    "        'input_dim': 21,\n",
    "        'hidden_dims': [512] * 4,\n",
    "        'gpu': 0,\n",
    "    },\n",
    "    'optimizer_kwargs': {'lr': 2e-3},\n",
    "    'task_kwargs': {\n",
    "        'normalization': False,\n",
    "        'num_mlp_layer': 2,\n",
    "        'metric': ['mcc'],\n",
    "    },\n",
    "    'batch_size': 1,\n",
    "    'gradient_interval': 1,\n",
    "    'verbose': False,\n",
    "    'valid_fold_num': 0,\n",
    "    'dataset_kwargs': {\n",
    "        'to_slice': True,\n",
    "        'max_slice_length': 550,\n",
    "        'padding': 100\n",
    "    },\n",
    "    'num_mlp_layer': 2,\n",
    "}\n",
    "pipeline = Pipeline(**pipeline_kwargs)\n",
    "\n",
    "data.dataloader.graph_collate = graph_collate_with_gvp\n",
    "train_record = pipeline.train_until_fit(max_epoch=10, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init pipeline, model: gvp, dataset: atpbind3d-minimal, gpus: [0]\n",
      "load model gvp, kwargs: {'node_in_dim': (6, 3), 'node_h_dim': (100, 16), 'edge_in_dim': (32, 1), 'edge_h_dim': (32, 1), 'num_layers': 1, 'drop_rate': 0.1, 'output_dim': 20, 'gpu': 0}\n",
      "get dataset with kwargs: {'to_slice': True, 'max_slice_length': 550, 'padding': 100}\n",
      "Initialize Undersampling: all ones\n",
      "Initialize Weighting: all ones\n",
      "train samples: 4, valid samples: 2, test samples: 5\n",
      "no scheduler\n",
      "pipeline batch_size: 1, gradient_interval: 1\n",
      "0m1s {'mcc': 0.1044, 'valid_mcc': 0.1392, 'train_bce': 0.4086, 'valid_bce': 0.1612, 'best_threshold': -2.7}\n",
      "0m1s {'mcc': 0.1007, 'valid_mcc': 0.144, 'train_bce': 0.2615, 'valid_bce': 0.1574, 'best_threshold': -3.0}\n",
      "0m1s {'mcc': 0.0542, 'valid_mcc': 0.1363, 'train_bce': 0.2624, 'valid_bce': 0.1593, 'best_threshold': -3.0}\n",
      "0m1s {'mcc': 0.0871, 'valid_mcc': 0.1718, 'train_bce': 0.2591, 'valid_bce': 0.1663, 'best_threshold': -2.7}\n",
      "0m1s {'mcc': 0.0932, 'valid_mcc': 0.1439, 'train_bce': 0.2541, 'valid_bce': 0.1681, 'best_threshold': -2.6}\n",
      "0m1s {'mcc': 0.0702, 'valid_mcc': 0.157, 'train_bce': 0.2575, 'valid_bce': 0.1693, 'best_threshold': -2.7}\n",
      "0m1s {'mcc': 0.0925, 'valid_mcc': 0.1458, 'train_bce': 0.2543, 'valid_bce': 0.1568, 'best_threshold': -3.0}\n",
      "0m1s {'mcc': 0.0529, 'valid_mcc': 0.1468, 'train_bce': 0.2487, 'valid_bce': 0.1571, 'best_threshold': -2.1}\n",
      "0m1s {'mcc': 0.0776, 'valid_mcc': 0.1479, 'train_bce': 0.2461, 'valid_bce': 0.1578, 'best_threshold': -2.4}\n",
      "0m1s {'mcc': 0.0561, 'valid_mcc': 0.1525, 'train_bce': 0.248, 'valid_bce': 0.1514, 'best_threshold': -2.4}\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "pipeline_kwargs = {\n",
    "    'model': 'gvp',\n",
    "    'dataset': 'atpbind3d-minimal',\n",
    "    'gpus': [0],\n",
    "    'model_kwargs': {\n",
    "        'node_in_dim': (6, 3),\n",
    "        'node_h_dim': (100, 16),\n",
    "        'edge_in_dim': (32, 1),\n",
    "        'edge_h_dim': (32, 1),\n",
    "        'num_layers': 1,\n",
    "        'drop_rate': 0.1,\n",
    "        'output_dim': 20,\n",
    "        'gpu': 0,\n",
    "    },\n",
    "    'task_kwargs': {\n",
    "        'normalization': False,\n",
    "        'num_mlp_layer': 1,\n",
    "        'metric': ['mcc'],\n",
    "        'node_feature_type': 'gvp_data',\n",
    "    },\n",
    "    'batch_size': 1,\n",
    "    'gradient_interval': 1,\n",
    "    'verbose': False,\n",
    "    'valid_fold_num': 0,\n",
    "    'dataset_kwargs': {\n",
    "        'to_slice': True,\n",
    "        'max_slice_length': 550,\n",
    "        'padding': 100\n",
    "    },\n",
    "    'num_mlp_layer': 2,\n",
    "}\n",
    "logging.basicConfig(level=logging.DEBUG)\n",
    "pipeline = Pipeline(**pipeline_kwargs)\n",
    "\n",
    "data.dataloader.graph_collate = graph_collate_with_gvp\n",
    "train_record = pipeline.train_until_fit(max_epoch=10, patience=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence: IGKGFEDLMT\n",
      "prediction: tensor([[-3.4665],\n",
      "        [-1.8042],\n",
      "        [-1.8622],\n",
      "        [-2.1003],\n",
      "        [-2.7256],\n",
      "        [-2.9655],\n",
      "        [-3.0620],\n",
      "        [-3.5202],\n",
      "        [-2.1327],\n",
      "        [-2.1367]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "label: tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "from lib.utils import protein_to_sequence\n",
    "\n",
    "\n",
    "def protein_to_sequence(protein):\n",
    "    seq = protein.to_sequence()\n",
    "    if isinstance(seq, list):\n",
    "        seq = seq[0]\n",
    "    return ''.join(i for i in seq if i != '.')\n",
    "\n",
    "# Extract the task object from the pipeline\n",
    "task = pipeline.task\n",
    "\n",
    "# Get a batch from the train set\n",
    "train_loader = data.DataLoader(\n",
    "    pipeline.train_set,\n",
    "    batch_size=pipeline.batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=graph_collate_with_gvp\n",
    ")\n",
    "\n",
    "# Get the first batch\n",
    "batch = next(iter(train_loader))\n",
    "\n",
    "# Move the batch to the correct device\n",
    "batch = utils.cuda(batch, device=torch.device(f'cuda:{pipeline.gpus[0]}'))\n",
    "print(f'sequence: {protein_to_sequence(batch[\"graph\"])[29:39]}')\n",
    "print(f'prediction: {task.predict(batch)[29:39]}')\n",
    "print(f'label: {batch[\"graph\"].target[29:39]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "for i in range(100):\n",
    "    task.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    optimizer.zero_grad()\n",
    "    loss, metric = task(batch)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sequence: IGKGFEDLMT\n",
      "prediction: tensor([[-3.4665],\n",
      "        [-1.8042],\n",
      "        [-1.8622],\n",
      "        [-2.1003],\n",
      "        [-2.7256],\n",
      "        [-2.9655],\n",
      "        [-3.0620],\n",
      "        [-3.5202],\n",
      "        [-2.1327],\n",
      "        [-2.1367]], device='cuda:0', grad_fn=<SliceBackward0>)\n",
      "label: tensor([[1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [1],\n",
      "        [0],\n",
      "        [0],\n",
      "        [0],\n",
      "        [1],\n",
      "        [0]], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "task.eval()\n",
    "print(f'sequence: {protein_to_sequence(batch[\"graph\"])[29:39]}')\n",
    "print(f'prediction: {task.predict(batch)[29:39]}')\n",
    "print(f'label: {batch[\"graph\"].target[29:39]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
